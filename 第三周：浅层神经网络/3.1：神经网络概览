
在第二周的课程讨论了logistic回归，


了解了上面模型和下面流程图的联系。我们需要输入特征 [公式] ，参数 [公式] ，用以计算 [公式] ，再将 [公式] 作为因变量输入到logistic函数得到 [公式] ，我们用 [公式] 同时表示输出 [公式] ,接下来就可以计算损失函数 [公式] 。


以上是一个sigmoid单元。我们可以把很多sigmoid单元堆叠起来构成一个神经网络。志气一个节点对应两个计算步骤即：先计算出 [公式] 值然后再计算出 [公式] 值。


为表明符号的含义，我们稍后会使用如下记号：

[公式] 表示输入特征，参数为 [公式] ，这样我们可以计算出 [公式] 。

尖括号表示与这些节点相关的量，或也成为层。[1]表示第一层，[2]表示第二层。以此和圆括号表示的单个训练样本进行区分。


实际上我们可以不只一层。如上两层计算可表示为：


其流程为，使用类似logistic回归计算 [公式] ,再使用 [公式] 计算 [公式] ,接下来用另外一个线性方程计算 [公式] ，接着计算出 [公式] 。 [公式] 就是整个神经网络的最终输出。

在logistic回归中会计算导数 [公式] ，同样在神经网络中，也有类似的反向计算。


因公式格式问题，完整博文详见知乎：https://zhuanlan.zhihu.com/p/408362129
