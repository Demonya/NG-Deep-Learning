上一节中，学习了如何计算导数和把梯度下降法应用到logistic回归的一个训练样本上,因为训练集是由m个样本组成的，我们尝试把梯度下降法应用到m个训练样本上。

成本函数 [公式] ，

其中 [公式]

对于任意单个训练样本计算导数：Using single training example [公式]

[公式]

全局成本函数是一个求和,实际上是1到m项损失函数和的平均，它表明全局成本函数对 [公式] 的导数也同样是各项损失函数对 [公式] 导数的平均：

[公式]


因此我们真正需要做的是计算每个单样本的导数求和之后再求平均,这会得到全局梯度值，能够把它直接应用到梯度下降算法中。

仍以logistic回归算法为例,初始化： [公式]

使用for循环遍历数据集,同时计算相应的每个训练样本的导数然后求和：(以两个特征为例故参数仅有 [公式] )

[公式]

循环对所有的m个训练样本都进行了计算，因此需要除计算均值：

[公式]

以上计算了全局成本函数J对各个参数 [公式] 的导数,梳理下细节:

我们使用 [公式] 作为累加器,所以在这些计算后， [公式] 等于全局成本函数 [公式] ,对 [公式] 也同理。同时需要注意的是 [公式] 没有上标 [公式] ，因为我们在这代码中把它们作为累加器，去求取整个训练集上的和,相反的 [公式] 是对应于单个训练样本的 [公式] ,上标 [公式] 对应第 [公式] 个训练样本的计算。

完成以上所有计算后，应用一步梯度下降：

[公式]


以上所有计算表明计算中有两个缺点，需要编写两个for循环,第一个for循环是遍历m个训练样本的小循环。
第二个for循环是遍历所有的特征的for循环。在深度学习算法中，在代码中显示地使用for循环会使算法很低效，且深度学习算法的数据集越来越大，
所以能够使用其他加速计算代替显示for循环计算是很重要的。向量化技术可以替代显示for循环。后续几节将会讲解向量化。

因公式格式问题,完整博文详见知乎:https://zhuanlan.zhihu.com/p/406979746
