本节讲述怎样计算偏导数来实现logistic回归中的梯度下降法。核心关键是其中有几个重要的公式用来实现logistic回归的梯度下降。本讲仍将使用导数流程图来计算梯度。事实上用导数流程图计算logistic回归的梯度下降有点大材小用。但以这种方式进行讲解可以更好的理解梯度下降。从而可以更深刻而全面地理解神经网络。
Recap:
   
   
   
假设数据集仅有两个特征  ，为了计算   ，我们需要输入参数   。
在logistic回归中，我们需要做的是变换参数   的值来最小化损失函数。在前面我们使用前向传播步骤在单个训练样本上计算损失函数，之后需要讨论如何向后计算偏导数。

要想计算损失函数L的导数,首先我们要向前一步先计算损失函数的导数   
   
                     

计算出   后,可以再向后一步计算   
   
其中   则，
   
         
         
         
带入   
                            
现在向后传播的最后一步，计算看看   需要如何变化,特别地关于   的导数   
   
                     
同理可得dw1：\frac{dL}{dw1} 
 \frac{dL(a,y)}{dw1} = \frac{dL(a,y)}{da} * \frac{da}{dz}*\frac{dz}{dw1} 
 = (a-y)*x1 
同理可得dw2：\frac{dL}{dw2} 
 \frac{dL(a,y)}{dw2} = \frac{dL(a,y)}{da} * \frac{da}{dz}*\frac{dz}{dw2} 
 = (a-y)*x2 
当对参数更新时有：
   
   
   

以上就是单个样本进行梯度下降更新参数的过程,但训练logistic回归模型不仅仅只有一个训练样本，
而是有m个训练样本的整个训练集，下一讲继续介绍以上想法如何应用到整个训练集。
公式格式问题,完整博文详见知乎:https://zhuanlan.zhihu.com/p/406726784
