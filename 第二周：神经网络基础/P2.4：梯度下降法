损失函数：衡量单一训练样例的效果

成本函数：用于衡量参数w和b在全部训练集上的效果

使用梯度下降法来训练或学习训练集上的参数w和b。

Gradient Descent:

Recap: [公式] 被定义为平均值,即1/m的损失函数之和。

[公式]

[公式]

损失函数可以衡量算法的效果,每一个训练样例都输出对应的 [公式] ,把它与真实标签 [公式] 进行比较。成本函数衡量了参数 [公式] 在训练集上的效果。要学习到合适的参数 [公式] ,很自然地就想到需要找到使得成本函数 [公式] 尽可能小的 [公式] 。


图中的横轴表示空间参数 [公式] 在实践中 [公式] 可以是更高维的,为了画图方便,我们让 [公式] 是一个实数， [公式] 也是一个实数。成本函数 [公式] 是在水平轴 [公式] 上的曲面,曲面的高度表示了 [公式] 在某一点的值,我们想做的就是找到使得成本函数取得最小时对应的参数 [公式] 。

可以看到成本函数 [公式] 是一个凸函数,这也是logistic回归选取该成本函数的重要原因之一。为了找到更好的参数,我么随机初始化 [公式] ，对凸函数而言任意的初始化方法都是有效的，通过梯度下降最终会找到最优参数,使得成本函数最小。通常使用0进行初始化。但是对于logistic回归我们通常不这样初始化参数。

梯度下降法所做的就是从初始点开始朝最陡的下坡方向走一步，直至找到全局最优解。为了更好的说明梯度下降法,我们通过以下例子来说明(忽略参数b)。


梯度下降将重复执行以下的更新操作：

[公式]

其中 [公式] 表示学习率,可以控制每一次迭代或者梯度下降法中的步长。其次导数就是对参数 [公式] 的更新或者变化量。写代码时通常约定 [公式] 表示导数。

导数的定义是函数在这个点的斜率,而函数的斜率是高除以宽。


当初始点在右侧时,斜率为正更新时:w减正数逐渐变小。当初始点在左侧时,斜率为负更新时:w减负数逐渐变大。不论初始化点在左侧还是右侧最终会收敛到局部最优解。

以上是仅有参数 [公式] 的示例。在logistic回归中成本函数是含有 [公式] 两个参数的函数。因此更新时有：

[公式] [公式]
