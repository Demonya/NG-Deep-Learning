上一节中学习了logistic回归模型,为了训练logistic回归模型的参数w、b,需要定义一个成本函数（Cost Function）。
\hat{y} = \sigma(w^{T} * x + b) , where \sigma(z) = \frac{1}{1+e^{-z}}  输入经过sigmoid函数将线性结果映射到0-1之间的某个概率值
为了让模型通过学习调整参数,需要给定一个m个样本的训练数据集:
Given {(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}),  ...  (x^{(m)},y^{(m)})}, want \hat{y}^{i} \approx y^{i}
经过训练得到参数w、b,并希望\hat{y}^{i} 与 y^{i}十分接近。
需要说明：\hat{y}是对一个训练样本x来说的,对于每个训练样本使用带有圆括号的上标来表示具体样本特征和样本标签,也方便引用说明和区分样本。
训练样本(i)对应的预测值是\hat{t}^{i},它是通过sigmoid函数作用到W^{T} * x + b 得到的。也可以有如下定义：
z^{i} = w^{T} * X^{i} + b 
带有上标(i)表示x、y、z和第i个训练样本有关。

我们可以定义Loss(error) Function：L(\hat{y},y) = square(\hat{y} - y),但通常在logistic回归中大家都不这样做。
因为当学习这些参数时,之后讨论的优化问题会变成非凸的,最后会得到很多个局部最优解,梯度下降法可能找不到全局最优值。
损失函数直观理解就是：通过定义的损失函数L来衡量预测输出值\hat{y}与y的实际值有多接近。
均方误差看似是一个合理的损失函数,但梯度下降法不好用。在logistic回归中我们会定义一个不同的损失函数(交叉熵),他起着与均方误差相似的作用,
但会给我们一个凸的优化问题,容易去做优化找到全局最优解。

logistic损失函数：
L(\hat{y},y) = - ylog(\hat{y}) - (1-y)log(1-\hat{y})
直观理解为何这个损失函数能够起作用,我们希望的是损失函数尽可能的小：考虑以下两种情况：
1) if y=1 : L(\hat{y},y) = - log(\hat{y}) <---  want log(\hat{y}) large, want \hat{y} large 
如果要损失函数越小越好,当y=1时损失函数仅等于-log(\hat{y}),则需要log(\hat{y})越大,即\hat{y}越大。
但sigmoid函数得出的结果永远不会大于1,也就是说当y=1时,会想让\hat{y}尽可能的大才能保证Loss Function尽可能的小。所以需要\hat{y}接近1。
2) if y=0 : L(\hat{y},y) = - log(1-\hat{y}) <---  want log(1-\hat{y}) large, want \hat{y} small 
如果要损失函数越小越好,当y=0时损失函数仅等于-log(1- \hat{y}),则需要log(1-\hat{y})越大,即\hat{y}越小。
但sigmoid函数得出的结果永远不会小于0,也就是说当y=0时,会想让1-\hat{y}尽可能的大才能保证Loss Function尽可能的小。所以需要\hat{y}接近0。
有很多损失函数都能达到上述结果；即如果y=1尽量让\hat{y}很大(接近1),如果y=0尽量让\hat{y}很小(接近0)。

损失函数是在单个训练样本中定义的,它衡量了参数在单个训练样本上的表现。成本函数是在整个训练集上的表现。
Cost Function：
J(w,b) = \frac{1}{m}\sum_{i=1}^{m}L(\hat{y}^{i},y^{i})
       =- \frac{1}{m}\sum_{i=1}^{m}[y^{i}log(\hat{y}^{i}) + (1-y^{i})log(1-\hat{y}^{i})]
J(w,b)是所有训练样本的损失函数和,而\hat{y}是用一组特定的参数w、b,通过logistic回归算法得出的预测输出值。
logistic回归可以被看作是一个非常小的神经网络。
