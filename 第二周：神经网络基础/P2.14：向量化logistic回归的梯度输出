在之前的视频中，我们知道了如何通过向量化同时计算整个训练集预测值，本节将学习如何向量化计算m个训练数据的梯度，强调一下是同时计算。我们会将之前讲的结合起来展现如何得到一个非常高效的logistic回归的实现。

讲解梯度下降时，曾列举了几个例子：

[公式]

对m个训练数据做同样的运算。我们现在可以定义一个新的变量：

[公式]

所有的dz变量横向排列，dz将会是一个1*m的矩阵或者说一个m维的列向量。

我们之前已经描述了如何计算：

[公式]

也定义了：

[公式] ，像之前一样横向排列进行堆叠形成矩阵形式。

基于以上的定义，我们可以这样计算 [公式]

在之前的实现中，我们已经去掉了一个for循环，但是仍然有一个遍历训练集的循环。

初始化 [公式] 计算 [公式]

累加计算完成后再求均值： [公式] ,同理b也类似。

对 [公式] 进行向量化处理有：

[公式]

[公式]


对logistic整体算法进行向量化：没有向量化的显示for循环： [公式]

使用向量化方法进行改写：

[公式]

以上实现了logistic回归的梯度下降一次迭代。尽管有些情况我们可以将显示for循环使用numpy进行处理，但有些情形是没办法进行替换的。
例如当我们希望多次迭代进行梯度下降，那么仍需使用for循环。

因公式格式原因，完整博文详见知乎：https://zhuanlan.zhihu.com/p/407657551
