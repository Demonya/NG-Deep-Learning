已知的输入特征向量x可能是一张图,希望识别出该图片中是否有猫。需要一个算法,可以给出预测值:Given x -> want \hat{y} 
其中\hat{y}就是对y的预测,更准确的说\hat{y}是一个概率：\hat{y} = P(y=1|x) 当输入特征x满足条件时y就是1
类比输入图片是否有猫的例子,当我们输入一张图片（特征向量x）希望通过算法给出该图片中有猫的概率值。
x是n_{x}维向量,logistic回归的参数是w也是n_{x}维向量,b是一个实数。
已知输入x和参数w、b,如何计算输出预测y_{hat}?

问题描述为：
Question:Given X ,want  \hat{y} = P(y=1|x)  0 <= \hat{y} <= 1 
Input:x \in \Re^{n_{x}}
Parameters:w \in \Re^{n_{x}}, b \in \Re
Output: \hat{y} = \sigma( w^{T} * x + b )

输出可以输入经过任意的函数得到的结果,
尝试线性回归假设\hat{y}是输入x的线性函数,但这不是一个非常好的二元分类方法,因为我们希望\hat{y}是y=1的概率,\hat{y}应该介于0-1之间。
但如果假设为线性函数很难实现,因为w^{t} * x + b 可能比1大的多,或者是负值,这样的概率没有任何意义。
所以在logistic回归中,我们的输出\hat{y} = sigmoid( w^{T} * x + b ) 即输入经过sigmoid函数得到输出。
设z = w^{T} * x + b, 
\sigma(z) = \frac{1}{1+e^{-z}}: 
if z large \sigma(z) = 1   如果z很大 e的-z次方趋向于0,分母趋向于1,整体等于1 
if z small \sigma(z) = 0   如果z很小,e的-z次方趋向于无穷,分母无穷整体为0

当我们要使用logistic回归是需要做的就是学习参数w、b, \hat{y}会是对y=1概率比较好的估计。

符号约定：
本课程中通常会把w、b参数分开,b对应一个拦截器。
其他课程中可能有对b进行整合为一个向量的表示方法：
x_{0} = 1   w \in \Re^{n_{x+1}}  \hat{y} = \sigma(\theta^{T} * x)  
Θ = [Θ_{0} Θ_{1} ... Θ_{n_{x}} ]^{T},
其中b = Θ_{0}, w = Θ_{1} ... Θ_{n_{x}} 
事实上在实现神经网络时,将b和w看作独立的参数可能更好。
